# Real Time Log Data Analtyics using Lambda Architecture  , Spark and Kafka

### Log Analysis

Log analysis is the process of reviewing, interpreting and understand computer-generated records called logs. Logs are generated by a range of programmable technologies, including networking devices, operating systems, applications, and more. A log consists of a series of messages in time-sequence that describe activities going on within a system. Log files may be streamed to a log collector through an active network, or they may be stored in files for later review for batch processsing.It is the subtle technique of evaluating and interpreting these messages in order to get insights into any system's underlying functioning.

There can be different types of logs which can be monitored such as System activity logs , Networking logs , Technical logs , and Cyber security monitoring logs.The information collected in web server logs can help us with Network troubleshooting efforts , Development and quality assurance, Identifying and understanding security issues, etc.

### Data Pipeline: 

It refers to a system for moving data from one system to another. The data may or may not be transformed, and it may be processed in real-time (or streaming) instead of batches. Right from extracting or capturing data using various tools, storing raw data, cleaning, validating data, transforming data into query worthy format, visualization of KPIs including Orchestration of the above process is data pipeline. 
Here, for this project we will be using Lambda Architecture which is  basically an system that provides the feature to process real time and batch data in the same pipeline.



### Objective of the project

To analyse the Real-time log data using the visualization web app by using multiple data streaming and processing technologies.This process involves launching an EC2 instance on AWS and installing Docker and other tools such as Apache Spark, Apache NiFi, Apache Kafka, Plotly, etc. The log data is extracted using Apache NiFi and Apache Kafka, transformed and loaded using Cassandra and HDFS and finally visualizing it using Python Plotly and Dash with the usage of graph and table app call-back. 

### Dataset used (with source link!)
NASA access log dataset
Link:  https://www.kaggle.com/datasets/souhagaa/nasa-access-log-dataset-1995


### Technologies Used

* Apache Spark 
* Apache NiFi
* Apache Kafka
* Docker
* Amazon EC2 Instance
* Cassandra
* HDFS
* Plotly
* Dash
* Jupyter Lab

### Architecture

<img width="989" alt="Screenshot 2022-07-19 at 2 34 32 AM" src="https://user-images.githubusercontent.com/25201417/179617142-0adb9b5a-bb51-4893-b6b4-a47c07e896f8.png">





Lambda Architecture provides the feature to process real time and batch data in the same pipeline. As shown in the diagram, it has 3 different layers namely Speed Layer , Batch Layer and Serving Layer. 
The speed layer  continuously streams real-time data using Kafka and provides to serving layer and similarly we can capture and store this data in cold storage like HDFS which can be further used for analysis.
The real-time latest data will be stored in Cassandra and can be visualised on live dashboard deployed on Dash framework and HDFS data from cold path can be grouped in hourly or daily data after processing and can be later used for visualisation.

### Environment Setup

#### AWS EC2 instance and Security Group creation

1. Login to AWS console and get t2.xlarge instance with atleast 32 GB of storage.
2. Allow all the ports in the range 4000 - 38888 for this instance.
3. Connect to ec2 via ssh ssh -i "D:\path\to\private\key.pem" user@Public_DNS
4. Apply Port forwarding in ordert to allow EC2 to connect to our local host 

>> ssh -i "path_to_pem_file\project_pem.pem" user@Public_DNS -L 2081:localhost:2041 -L 4888:localhost:4888 -L 2080:localhost:2080 -L 8050:localhost:8050 -L 4141:localhost:4141

5. Copy Docker compose .yml file and pem file to EC2 instance, using scp command.


<img width="1108" alt="Screenshot 2022-07-14 at 11 41 56 PM" src="https://user-images.githubusercontent.com/25201417/179055294-1425a6f3-df11-478c-bb53-9779aac94cce.png">




#### Docker: Install and Run


>> sudo yum install docker 


<img width="1108" alt="Screenshot 2022-07-14 at 11 55 58 PM" src="https://user-images.githubusercontent.com/25201417/179055699-76804300-c9f4-4542-aed2-ee163ed33a7d.png">


>> sudo systemctl start docker
>> List Docker containers running: docker ps

<img width="617" alt="Screenshot 2022-07-15 at 12 02 10 AM" src="https://user-images.githubusercontent.com/25201417/179056764-6d5e25b0-cd86-43f4-83ab-afc8446bf272.png">

>> Exectue a container in docker : docker exec -i -t <container_id> bash

>> Stop all containers: docker-compose stop


#### Domains to access Tools on Local (refer to docker-compose.yml file)

* NiFi : http://localhost:2080/nifi/
* Jupyter Lab : http://localhost:4888/lab
* HDFS : http://localhost:50070/
* Dash Application : http://localhost:8050/


<img width="968" alt="Screenshot 2022-07-15 at 12 09 44 AM" src="https://user-images.githubusercontent.com/25201417/179058335-091e0773-d078-4c36-b8bf-a75b87259945.png">

### Common Log Format

[Reference](https://en.wikipedia.org/wiki/Common_Log_Format#:~:text=For%20computer%20log%20management%2C%20the,when%20generating%20server%20log%20files.
)

Common Log Format in log file is as follows: host ident authuser [date] "request" status bytes

<img width="1153" alt="Screenshot 2022-07-15 at 12 21 40 AM" src="https://user-images.githubusercontent.com/25201417/179060582-c3eb62ad-8f9d-4986-a6f4-ee1ae08414fe.png">

#### Actual Log File data

![Screenshot 2022-07-15 at 12 22 58 AM](https://user-images.githubusercontent.com/25201417/179060873-4711440d-0731-44a6-bd83-2cc161362c02.png)


### Parsing the Log File 
(Refer to : log_preprocessing.ipynb)

This section involves parsing the log data file using regex expression, cleaning and preprocessing the data using Spark.


The log data in txt is loaded after starting the spark session and is further parsed using regular expression in order to store the data in a dataframe with proper field names.
This spark df is further verified to check if data is loaded into df in the expected format, fixing the records with Null values and converting the timestamp to UTC time zone.

<img width="990" alt="Screenshot 2022-07-15 at 12 36 15 AM" src="https://user-images.githubusercontent.com/25201417/179063400-8c101c77-3896-492a-8408-ab52a3c45ee0.png">

## Extraction of Data
####Nifi : Downloading DB and Dataflow process (to Kafka)

*Data Source :  https://www.kaggle.com/datasets/souhagaa/nasa-access-log-dataset-1995*
[Kaggle Dataset]( https://www.kaggle.com/datasets/souhagaa/nasa-access-log-dataset-1995)

1. First we will verify if our docker containers are up and running and then we will copy the downloaded data from Kaggle to Nifi container using the following cmd 
>> docker exec -i -t nifi bash 
>> mkdir -p nasa_logs && cp /<path_to_data>/data.csv nasa_logs/data.csv

2. After storing data inside Nifi Container, we will stream/ingest the data from source (i.e. Nifi container) and publish to target system (i.e Kafka Topic) using Nifi Processor and Connection.

3. Kafka Commands 

>> Get Kafka Container ID : docker ps

>> docker exec -i -t kafka bash 

>> Creation of topic named nasa_logs_demo : kafka-topics.sh --create --topic nasa_logs_demo --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:2181 

>> List topics : kafka-topics.sh --list --bootstrap-server localhost:29092 

>> Describe topic : kafka-topics.sh --describe --topic nasa_logs_demo --zookeeper zookeeper:2181 

>> Delete topic : kafka-topics.sh --delete --topic nasa_logs_demo --zookeeper zookeeper:2181 

>> Read data from topic :  kafka-console-consumer.sh --bootstrap-server localhost:29092 --topic nasa_logs_demo --from-beginning --max-messages 30 



<img width="956" alt="Screenshot 2022-07-15 at 1 05 53 AM" src="https://user-images.githubusercontent.com/25201417/179068605-9181dcb8-078f-4c9d-a091-50f2a40f0d5d.png">


### Nifi : Flow Setup and Kafka : Consume Data

In this section, we will stream the downloaded file in Nifi and real-time stream it to Kafka topic. For this process, we first access Nifi through http://localhost:2080/nifi/ 
In Nifi , we will add proccess group for different operations such as GetFile , SplitText , and PublishKafka_2_6
We can then configure the different parameters of the process groups such as tasks, schedules, and properties & finally we consume data in Kafka topic 
using read data command from previous section.


<img width="1245" alt="Screenshot 2022-07-15 at 1 13 46 AM" src="https://user-images.githubusercontent.com/25201417/179069882-bc4b253b-6cce-4360-b80a-b202f4ab3f41.png">


## Transformation and Loading of Data

Cassandra is a free and open-source, distributed, wide-column store, NoSQL database management system designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure.

Cassandra Commands:
>> docker exec -i -t cassandra bash
>>  cqlsh -u cassandra -p cassandra
>> CREATE KEYSPACE IF NOT EXISTS LogAnalysis WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};
>> CREATE TABLE IF NOT EXISTS LogAnalysis.NASALog (host text , time text , method text , url text , response text , bytes text, extension text, time_added text,PRIMARY KEY (host));
>> truncate table LogAnalysis.NASALog;
  
Create a folder in HDFS
>>  docker exec -i -t namenode bash
>> hdfs dfs -mkdir -p /output1/nasa_logs/
>> http://localhost:50070/

As we are streaming data from Nifi to Kafka, so we are going to get a line instead of fields. So, in order to get the fields , we have to split the line into multiple fields by setting up the scehma.

<img width="830" alt="Screenshot 2022-07-15 at 1 32 04 AM" src="https://user-images.githubusercontent.com/25201417/179073053-5ef038c4-b8be-4a5f-bff3-1deea37ec8e1.png">
Refer: log_listener.ipynb

Writing data into Cassandra and HDFS

We can setup both configs for Cassandra and HDFS in Spark to store both real-time and batch processed data , as shown in the script
<img width="917" alt="Screenshot 2022-07-15 at 1 35 19 AM" src="https://user-images.githubusercontent.com/25201417/179073611-a303fdce-2ddd-452d-a5ab-bf84b86a02b1.png">

Then, we can verify the data into Cassandra by executing the container as shown below:-

<img width="917" alt="Screenshot 2022-07-15 at 1 39 52 AM" src="https://user-images.githubusercontent.com/25201417/179074324-a196d26c-d111-419e-99be-05307ef4513f.png">

and for HDFS using the command :
>> hdfs dfs -ls /output/nasa_logs

## Data Visualisation 
Refer: log_visualizer.ipynb

In this section, as demonstrated in ipynb ,we wii retrieve  the data from Cassandra and HDFS using generic function, build a HTML dashboard in Dash application and for visualisation we will create 3 seperate dashboards - Realtime, Hourly and Daily Dashboard.

<img width="917" alt="Screenshot 2022-07-15 at 2 02 45 AM" src="https://user-images.githubusercontent.com/25201417/179078020-93a30e52-e790-489a-8f38-26c96caca1c8.png">

<img width="1309" alt="Screenshot 2022-07-15 at 2 04 20 AM" src="https://user-images.githubusercontent.com/25201417/179078281-f859fb9e-fddf-44e2-9911-b6b9ddbdc41d.png">

<img width="1318" alt="Screenshot 2022-07-15 at 2 05 38 AM" src="https://user-images.githubusercontent.com/25201417/179078474-62ae1c6e-2db9-487c-921b-db5ecd3035d6.png">


_______




